{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from jason file\n",
    "datastore = [json.loads(line) for line in open('C:/Users/Omar-ElQady/Downloads/Compressed/Sarcasm_Headlines_Dataset_v2.json', 'r')]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [] \n",
    "labels = []\n",
    "urls = []\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thirtysomething scientists unveil doomsday clock of hair loss\n",
      "1\n",
      "https://www.theonion.com/thirtysomething-scientists-unveil-doomsday-clock-of-hai-1819586205\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print(labels[0])\n",
    "print(urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 3000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making Tokenization as indexing its\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#converte text to sequence then padding its for training and testing sentences\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 325, 1, 1, 2489, 3, 655, 993]\n",
      "[1, 1723, 735, 2490, 47, 248, 11, 1824, 919, 8, 1825, 2032, 2297]\n",
      "\n",
      "[   1  325    1    1 2489    3  655  993    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "[   1 1723  735 2490   47  248   11 1824  919    8 1825 2032 2297    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "(20000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(training_sequences[0])\n",
    "print(training_sequences[1])\n",
    "print()\n",
    "print(training_padded[0])\n",
    "print(training_padded[1])\n",
    "print()\n",
    "print(training_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100)\n",
      "(20000,)\n",
      "(8619, 100)\n",
      "(8619,)\n"
     ]
    }
   ],
   "source": [
    "# Need this block to get it to work with TensorFlow 2.x\n",
    "import numpy as np\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)\n",
    "print(training_padded.shape)\n",
    "print(training_labels.shape)\n",
    "print(testing_padded.shape)\n",
    "print(testing_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding is a vector\"arrow\" points to dirction ex:if direction is left is bad, right is good ,top left:bad , top right:not bad \n",
    "#Embedding should define vector\"arrow\" for each word\n",
    "#embedding_dim is dim of arrows(vector) to use \n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 16)           48000     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 48,433\n",
      "Trainable params: 48,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "625/625 - 3s - loss: 0.6623 - accuracy: 0.6102 - val_loss: 0.5608 - val_accuracy: 0.7760\n",
      "Epoch 2/30\n",
      "625/625 - 2s - loss: 0.4470 - accuracy: 0.8074 - val_loss: 0.3900 - val_accuracy: 0.8285\n",
      "Epoch 3/30\n",
      "625/625 - 2s - loss: 0.3562 - accuracy: 0.8467 - val_loss: 0.3625 - val_accuracy: 0.8366\n",
      "Epoch 4/30\n",
      "625/625 - 2s - loss: 0.3242 - accuracy: 0.8616 - val_loss: 0.3509 - val_accuracy: 0.8446\n",
      "Epoch 5/30\n",
      "625/625 - 2s - loss: 0.3048 - accuracy: 0.8709 - val_loss: 0.3488 - val_accuracy: 0.8438\n",
      "Epoch 6/30\n",
      "625/625 - 2s - loss: 0.2910 - accuracy: 0.8770 - val_loss: 0.3506 - val_accuracy: 0.8427\n",
      "Epoch 7/30\n",
      "625/625 - 3s - loss: 0.2821 - accuracy: 0.8796 - val_loss: 0.3527 - val_accuracy: 0.8427\n",
      "Epoch 8/30\n",
      "625/625 - 2s - loss: 0.2763 - accuracy: 0.8826 - val_loss: 0.3594 - val_accuracy: 0.8407\n",
      "Epoch 9/30\n",
      "625/625 - 3s - loss: 0.2687 - accuracy: 0.8864 - val_loss: 0.3648 - val_accuracy: 0.8407\n",
      "Epoch 10/30\n",
      "625/625 - 2s - loss: 0.2645 - accuracy: 0.8870 - val_loss: 0.3691 - val_accuracy: 0.8378\n",
      "Epoch 11/30\n",
      "625/625 - 2s - loss: 0.2596 - accuracy: 0.8892 - val_loss: 0.3827 - val_accuracy: 0.8337\n",
      "Epoch 12/30\n",
      "625/625 - 2s - loss: 0.2578 - accuracy: 0.8896 - val_loss: 0.3785 - val_accuracy: 0.8352\n",
      "Epoch 13/30\n",
      "625/625 - 2s - loss: 0.2558 - accuracy: 0.8936 - val_loss: 0.3799 - val_accuracy: 0.8376\n",
      "Epoch 14/30\n",
      "625/625 - 2s - loss: 0.2532 - accuracy: 0.8935 - val_loss: 0.3918 - val_accuracy: 0.8335\n",
      "Epoch 15/30\n",
      "625/625 - 2s - loss: 0.2518 - accuracy: 0.8931 - val_loss: 0.4114 - val_accuracy: 0.8277\n",
      "Epoch 16/30\n",
      "625/625 - 1s - loss: 0.2509 - accuracy: 0.8936 - val_loss: 0.3906 - val_accuracy: 0.8350\n",
      "Epoch 17/30\n",
      "625/625 - 1s - loss: 0.2493 - accuracy: 0.8950 - val_loss: 0.3973 - val_accuracy: 0.8347\n",
      "Epoch 18/30\n",
      "625/625 - 1s - loss: 0.2464 - accuracy: 0.8952 - val_loss: 0.4022 - val_accuracy: 0.8330\n",
      "Epoch 19/30\n",
      "625/625 - 1s - loss: 0.2474 - accuracy: 0.8963 - val_loss: 0.4067 - val_accuracy: 0.8322\n",
      "Epoch 20/30\n",
      "625/625 - 2s - loss: 0.2453 - accuracy: 0.8964 - val_loss: 0.4048 - val_accuracy: 0.8291\n",
      "Epoch 21/30\n",
      "625/625 - 1s - loss: 0.2454 - accuracy: 0.8974 - val_loss: 0.4042 - val_accuracy: 0.8305\n",
      "Epoch 22/30\n",
      "625/625 - 1s - loss: 0.2442 - accuracy: 0.8975 - val_loss: 0.4106 - val_accuracy: 0.8312\n",
      "Epoch 23/30\n",
      "625/625 - 2s - loss: 0.2440 - accuracy: 0.8981 - val_loss: 0.4162 - val_accuracy: 0.8296\n",
      "Epoch 24/30\n",
      "625/625 - 1s - loss: 0.2417 - accuracy: 0.8992 - val_loss: 0.4131 - val_accuracy: 0.8326\n",
      "Epoch 25/30\n",
      "625/625 - 1s - loss: 0.2416 - accuracy: 0.8982 - val_loss: 0.4496 - val_accuracy: 0.8216\n",
      "Epoch 26/30\n",
      "625/625 - 1s - loss: 0.2426 - accuracy: 0.8989 - val_loss: 0.4341 - val_accuracy: 0.8252\n",
      "Epoch 27/30\n",
      "625/625 - 2s - loss: 0.2420 - accuracy: 0.8986 - val_loss: 0.4183 - val_accuracy: 0.8304\n",
      "Epoch 28/30\n",
      "625/625 - 1s - loss: 0.2409 - accuracy: 0.8991 - val_loss: 0.4172 - val_accuracy: 0.8310\n",
      "Epoch 29/30\n",
      "625/625 - 1s - loss: 0.2394 - accuracy: 0.9006 - val_loss: 0.4198 - val_accuracy: 0.8284\n",
      "Epoch 30/30\n",
      "625/625 - 1s - loss: 0.2392 - accuracy: 0.9014 - val_loss: 0.4215 - val_accuracy: 0.8297\n"
     ]
    }
   ],
   "source": [
    "#to match word with label\n",
    "num_epochs = 30\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7632456 ]\n",
      " [0.88321066]\n",
      " [0.0025298 ]]\n"
     ]
    }
   ],
   "source": [
    "#Ugliness can be fixed, stupidity is forever\n",
    "#Zombies eat brains. You’re safe\n",
    "#Aim at nothing–you’ll hit it every time\n",
    "\n",
    "sentence = [\"Zombies eat brains. You’re safe\",\"Aim at nothing–you’ll hit it every time\",\"my name is omar and i have 21 yeas \"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
